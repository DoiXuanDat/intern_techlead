{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6aeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8ded18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a5905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_10832\\3790596850.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(\n",
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_10832\\3790596850.py:7: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"qwen3:1.7b\",  \n",
    "    temperature=0,\n",
    "    format=\"json\",         \n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"all-minilm:l6-v2\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15ca7d",
   "metadata": {},
   "source": [
    "# Logical routing\n",
    "\n",
    "doc: https://python.langchain.com/docs/use_cases/query_analysis/techniques/routing#routing-to-multiple-indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd385035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert programming question router.\n",
    "\n",
    "    Classify the following question into one of the data sources below:\n",
    "    - \"python_docs\": For questions about Python.\n",
    "    - \"js_docs\": For questions about JavaScript.\n",
    "    - \"golang_docs\": For questions about Golang.\n",
    "\n",
    "    OUTPUT REQUIREMENT:\n",
    "    Return ONLY a JSON object with the key \"datasource\". Do not provide any extra explanation.\n",
    "    Example: {{ \"datasource\": \"python_docs\" }}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "question_router = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb8f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = question_router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724384be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datasource': 'python_docs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c80b742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['datasource']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c951a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    datasource = result.get(\"datasource\", \"\").lower()\n",
    "    \n",
    "    if \"python_docs\" in datasource:\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in datasource:\n",
    "        return \"chain for js_docs\"\n",
    "    elif \"golang_docs\" in datasource:\n",
    "        return \"chain for golang_docs\"\n",
    "    else:\n",
    "        return \"chain for default\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = question_router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf22501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a68ee",
   "metadata": {},
   "source": [
    "# Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e53a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5ed7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embedding.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad9b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "{\n",
      "  \"answer\": \"A black hole is a region in space where the gravitational pull is so strong that not even light can escape. It forms when a massive star collapses under its own gravity, creating a singularity (a point of infinite density) surrounded by an event horizon, the boundary from which nothing can return. Black holes can vary in size, from the smallest (stellar black holes) to the largest (supermassive black holes at the centers of galaxies). They are invisible directly but their effects—like gravitational lensing or accretion disks—can be observed.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embedding.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf8218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
