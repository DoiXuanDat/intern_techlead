{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c800eee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xgear\\miniconda3\\envs\\intern\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_31604\\3321198127.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(\n",
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_31604\\3321198127.py:11: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# LLM cho chat\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"embeddinggemma:300m\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca1325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "from rich.pretty import Pretty\n",
    "from rich.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca5845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5630e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=articles,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1beedd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbff512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5156d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d0188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8b81fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 174)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = InMemoryVectorStore(embedding)\n",
    "doc_ids = vectorstore.add_documents(documents=splits)\n",
    "len(doc_ids), len(vectorstore.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d1c737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b865",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import chain\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5449af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "grading_prompt_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score to indicate whether the document is relevant to the question.\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\n",
    "User question:\n",
    "{question}\"\"\"\n",
    "query_rewriting_prompt_template = \"\"\"You a question re-writer that converts an input question to a better version that is optimized for web search. \n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "Here is the initial question:\n",
    "{question}\n",
    "\n",
    "Formulate an improved question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068435de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DocumentGrade(BaseModel):\n",
    "    chain_of_thought: str = Field(description=\"Reasoning\")\n",
    "    is_required: bool = Field(description=\"Is Document required\")\n",
    "\n",
    "class DocumentGrade(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to check if the document is relevant to the question\"\n",
    "    )\n",
    "    is_relevant: bool = Field(\n",
    "        description=\"Document is relevant to the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "@chain\n",
    "def grade_document(document, question):\n",
    "    grading_prompt = f\"\"\"\n",
    "    You are a strict document relevance grader.\n",
    "    Return a JSON in this exact format:\n",
    "\n",
    "    {{\n",
    "        \"chain_of_thought\": \"...\",\n",
    "        \"is_relevant\": true or false\n",
    "    }}\n",
    "\n",
    "    DOCUMENT:\n",
    "    {document}\n",
    "\n",
    "    QUESTION:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    response = llm.invoke(grading_prompt)\n",
    "\n",
    "    content = response.content.strip().strip(\"```json\").strip(\"```\")\n",
    "    return DocumentGrade(**json.loads(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66bd46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def WebSearchQuery(question):\n",
    "    prompt = f\"\"\"\n",
    "    You are a web search query optimizer.\n",
    "\n",
    "    Return ONLY a JSON object in this format:\n",
    "    {{\n",
    "        \"chain_of_thought\": \"your reasoning\",\n",
    "        \"web_search_query\": \"optimized query\"\n",
    "    }}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "\n",
    "    raw = resp.content if hasattr(resp, \"content\") else resp\n",
    "\n",
    "    cleaned = (\n",
    "        raw.replace(\"```json\", \"\")\n",
    "           .replace(\"```\", \"\")\n",
    "           .strip()\n",
    "    )\n",
    "    return json.loads(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49279d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_31604\\2226099129.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=4)\n",
      "C:\\Users\\xgear\\AppData\\Local\\Temp\\ipykernel_31604\\2226099129.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  rprint(web_search_tool.input_schema.schema())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Input for the Tavily tool.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'search query to look up'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Query'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'TavilyInput'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'description'\u001b[0m: \u001b[32m'Input for the Tavily tool.'\u001b[0m,\n",
       "    \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'description'\u001b[0m: \u001b[32m'search query to look up'\u001b[0m, \u001b[32m'title'\u001b[0m: \u001b[32m'Query'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'query'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'TavilyInput'\u001b[0m,\n",
       "    \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "web_search_tool = TavilySearchResults(k=4)\n",
    "rprint(web_search_tool.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5932aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> list[str]:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eeea230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    documents: list[Document]\n",
    "    grades: list[DocumentGrade]\n",
    "    is_web_search_required: bool\n",
    "    web_search_query: str\n",
    "    context: Annotated[list[Document], operator.add]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2d7477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def grade_documents(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    grades = grade_document.batch(\n",
    "        documents, question=question\n",
    "    )\n",
    "    filtered_documents = [document for (document, grade) in zip(documents, grades) if grade.is_relevant]\n",
    "    is_web_search_required = len(filtered_documents) < len(documents)\n",
    "            \n",
    "    return {\"context\": filtered_documents, \"grades\": grades, \"is_web_search_required\": is_web_search_required}\n",
    "\n",
    "\n",
    "def check_documents_relevance(state: State) -> Literal[\"rewrite_query\", \"generate_answer\"]:\n",
    "    is_web_search_required = state[\"is_web_search_required\"]\n",
    "\n",
    "    if is_web_search_required:\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        return \"generate_answer\"\n",
    "\n",
    "\n",
    "def rewrite_query(state: State):\n",
    "    question = state[\"question\"]\n",
    "    optimized = WebSearchQuery(question)\n",
    "    return {\"web_search_query\": optimized[\"web_search_query\"]}\n",
    "\n",
    "\n",
    "def web_search(state: State):\n",
    "    query = state[\"web_search_query\"]\n",
    "    results = web_search_tool.invoke({\"query\": query})\n",
    "    documents = [Document(page_content=result[\"content\"]) for result in results]\n",
    "    return {\"context\": documents}\n",
    "\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    docs_content = format_docs(state[\"context\"])\n",
    "    rag_prompt = rag_prompt_template.format(\n",
    "        question=state[\"question\"],\n",
    "        context=docs_content\n",
    "    )\n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=rag_prompt)\n",
    "    ])\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43c13802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000024DC7382870>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"rewrite_query\", rewrite_query)\n",
    "graph_builder.add_node(\"web_search\", web_search)\n",
    "graph_builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    check_documents_relevance\n",
    ")\n",
    "graph_builder.add_edge(\"rewrite_query\", \"web_search\")\n",
    "graph_builder.add_edge(\"web_search\", \"generate_answer\")\n",
    "graph_builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e36e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are common types of agent memory?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'78d81690-65bd-4ab1-901b-a640b3073c2f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Comparison of AD, ED, source policy and RL^2 on environments that require memory and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'950058e5-c85f-40e6-ab1a-6dfc6a619fa2'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sensory memory as learning embedding representations for raw inputs, including text, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation of information into a vector store database that can support fast maximum inner-product search </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a8569ff5-e126-4153-afcb-b68b5a24c9e0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Memory stream: is a long-term memory module (external database) that records a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model: surfaces the context to inform the agent’s behavior, according to relevance, recency and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">higher-level summaries of past events (&lt;- note that this is a bit different from self-reflection above)'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'e7b1ed62-8c0b-4325-a3d6-61cf8283c74d'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Short-term memory: I would consider all the in-context learning (See Prompt Engineering) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capability to retain and recall (infinite) information over extended periods, often by leveraging an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is missing from the model weights (often hard to change after pre-training), including current information, code </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">autonomous agent system.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'grades'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The document describes memory types in the human brain, specifically focusing on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sensory memory. It then mentions that these memory types are used in agent environments that require memory and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration, and that only binary reward is assigned. The question asks for the common types of agent memory, which</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is related to the context of the document.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The document discusses memory systems used in agents, specifically focusing on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">short-term memory, long-term memory, and the role of vector stores. It introduces MIPS, an algorithm for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approximate nearest neighbor search, which is beneficial for long-term memory. The question asks for common types </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of agent memory, which the document explicitly addresses.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The document describes an agent’s memory stream – a long-term database of agent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experience. It highlights the retrieval model’s process of scoring memory elements based on relevance, recency, and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance. The reflection mechanism suggests synthesizing past events into higher-level inferences. The question </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">asks for common types of agent memory, which is a foundational concept within this context.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The document discusses long-term memory, which is a key aspect of autonomous agent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems. It highlights the use of external vector stores and fast retrieval for retaining information over extended</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">periods. The document also mentions the agent learning to call external APIs to obtain missing information, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suggesting a reliance on short-term memory for initial learning and adaptation. Therefore, the question about </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">common types of agent memory is directly relevant to the document's discussion of long-term memory.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_web_search_required'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'78d81690-65bd-4ab1-901b-a640b3073c2f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Comparison of AD, ED, source policy and RL^2 on environments that require memory and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'950058e5-c85f-40e6-ab1a-6dfc6a619fa2'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sensory memory as learning embedding representations for raw inputs, including text, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation of information into a vector store database that can support fast maximum inner-product search </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a8569ff5-e126-4153-afcb-b68b5a24c9e0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Memory stream: is a long-term memory module (external database) that records a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model: surfaces the context to inform the agent’s behavior, according to relevance, recency and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">higher-level summaries of past events (&lt;- note that this is a bit different from self-reflection above)'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'e7b1ed62-8c0b-4325-a3d6-61cf8283c74d'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Short-term memory: I would consider all the in-context learning (See Prompt Engineering) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capability to retain and recall (infinite) information over extended periods, often by leveraging an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is missing from the model weights (often hard to change after pre-training), including current information, code </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">autonomous agent system.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Based on the provided text, here are the common types of agent memory described:\\n\\n*   **Sensory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Memory:** Retains impressions of sensory information.\\n*   **Short-Term Memory:** Limited context window, used for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning.\\n*   **Long-Term Memory:** External vector store for fast retrieval, accessible via fast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval.\\n*   **Maximum Inner Product Search (MIPS):** Stores embedding representations of information in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector database for fast search.\\n*   **Retrieval Model:** Surfaces context to inform the agent’s behavior.\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Reflection Mechanism:** Synthesizes memories into higher-level inferences.\\n*   **Short-Term Memory:** Utilizes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning to learn.\\n*   **Long-Term Memory:** Retains and recalls information over extended periods, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">leveraging an external vector store and fast retrieval.\\n\\n'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[32m'What are common types of agent memory?'\u001b[0m,\n",
       "    \u001b[32m'documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'78d81690-65bd-4ab1-901b-a640b3073c2f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Comparison of AD, ED, source policy and RL^2 on environments that require memory and \u001b[0m\n",
       "\u001b[32mexploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and \u001b[0m\n",
       "\u001b[32mDQN for watermaze.\u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Laskin et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nComponent Two: Memory#\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBig thank you to ChatGPT for \u001b[0m\n",
       "\u001b[32mhelping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my \u001b[0m\n",
       "\u001b[32mconversations with ChatGPT.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, \u001b[0m\n",
       "\u001b[32mretain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: \u001b[0m\n",
       "\u001b[32mThis is the earliest stage of memory, providing the ability to retain impressions of sensory information \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvisual, \u001b[0m\n",
       "\u001b[32mauditory, etc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. \u001b[0m\n",
       "\u001b[32mSubcategories include iconic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvisual\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, echoic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mauditory\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and haptic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtouch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'950058e5-c85f-40e6-ab1a-6dfc6a619fa2'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Sensory memory as learning embedding representations for raw inputs, including text, \u001b[0m\n",
       "\u001b[32mimage or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted \u001b[0m\n",
       "\u001b[32mby the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent \u001b[0m\n",
       "\u001b[32mcan attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m#\\nThe external \u001b[0m\n",
       "\u001b[32mmemory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding \u001b[0m\n",
       "\u001b[32mrepresentation of information into a vector store database that can support fast maximum inner-product search \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. To optimize the retrieval speed, the common choice is the approximate nearest neighbors \u001b[0m\u001b[32m(\u001b[0m\u001b[32mANN\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\u200b \u001b[0m\n",
       "\u001b[32malgorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge \u001b[0m\n",
       "\u001b[32mspeedup.\\nA couple common choices of ANN algorithms for fast MIPS:'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'a8569ff5-e126-4153-afcb-b68b5a24c9e0'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Memory stream: is a long-term memory module \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexternal database\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that records a \u001b[0m\n",
       "\u001b[32mcomprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly \u001b[0m\n",
       "\u001b[32mprovided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval \u001b[0m\n",
       "\u001b[32mmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and \u001b[0m\n",
       "\u001b[32mimportance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask \u001b[0m\n",
       "\u001b[32mLM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: \u001b[0m\n",
       "\u001b[32msynthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are \u001b[0m\n",
       "\u001b[32mhigher-level summaries of past events \u001b[0m\u001b[32m(\u001b[0m\u001b[32m<- note that this is a bit different from self-reflection above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'e7b1ed62-8c0b-4325-a3d6-61cf8283c74d'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Short-term memory: I would consider all the in-context learning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee Prompt Engineering\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mas utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the \u001b[0m\n",
       "\u001b[32mcapability to retain and recall \u001b[0m\u001b[32m(\u001b[0m\u001b[32minfinite\u001b[0m\u001b[32m)\u001b[0m\u001b[32m information over extended periods, often by leveraging an external \u001b[0m\n",
       "\u001b[32mvector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that\u001b[0m\n",
       "\u001b[32mis missing from the model weights \u001b[0m\u001b[32m(\u001b[0m\u001b[32moften hard to change after pre-training\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, including current information, code \u001b[0m\n",
       "\u001b[32mexecution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered \u001b[0m\n",
       "\u001b[32mautonomous agent system.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'grades'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The document describes memory types in the human brain, specifically focusing on \u001b[0m\n",
       "\u001b[32msensory memory. It then mentions that these memory types are used in agent environments that require memory and \u001b[0m\n",
       "\u001b[32mexploration, and that only binary reward is assigned. The question asks for the common types of agent memory, which\u001b[0m\n",
       "\u001b[32mis related to the context of the document.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The document discusses memory systems used in agents, specifically focusing on \u001b[0m\n",
       "\u001b[32mshort-term memory, long-term memory, and the role of vector stores. It introduces MIPS, an algorithm for \u001b[0m\n",
       "\u001b[32mapproximate nearest neighbor search, which is beneficial for long-term memory. The question asks for common types \u001b[0m\n",
       "\u001b[32mof agent memory, which the document explicitly addresses.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The document describes an agent’s memory stream – a long-term database of agent \u001b[0m\n",
       "\u001b[32mexperience. It highlights the retrieval model’s process of scoring memory elements based on relevance, recency, and\u001b[0m\n",
       "\u001b[32mimportance. The reflection mechanism suggests synthesizing past events into higher-level inferences. The question \u001b[0m\n",
       "\u001b[32masks for common types of agent memory, which is a foundational concept within this context.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m document discusses long-term memory, which is a key aspect of autonomous agent \u001b[0m\n",
       "\u001b[32msystems. It highlights the use of external vector stores and fast retrieval for retaining information over extended\u001b[0m\n",
       "\u001b[32mperiods. The document also mentions the agent learning to call external APIs to obtain missing information, \u001b[0m\n",
       "\u001b[32msuggesting a reliance on short-term memory for initial learning and adaptation. Therefore, the question about \u001b[0m\n",
       "\u001b[32mcommon types of agent memory is directly relevant to the document's discussion of long-term memory.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'is_web_search_required'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'78d81690-65bd-4ab1-901b-a640b3073c2f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Comparison of AD, ED, source policy and RL^2 on environments that require memory and \u001b[0m\n",
       "\u001b[32mexploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and \u001b[0m\n",
       "\u001b[32mDQN for watermaze.\u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Laskin et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nComponent Two: Memory#\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBig thank you to ChatGPT for \u001b[0m\n",
       "\u001b[32mhelping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my \u001b[0m\n",
       "\u001b[32mconversations with ChatGPT.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, \u001b[0m\n",
       "\u001b[32mretain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: \u001b[0m\n",
       "\u001b[32mThis is the earliest stage of memory, providing the ability to retain impressions of sensory information \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvisual, \u001b[0m\n",
       "\u001b[32mauditory, etc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. \u001b[0m\n",
       "\u001b[32mSubcategories include iconic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvisual\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, echoic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mauditory\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and haptic memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtouch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'950058e5-c85f-40e6-ab1a-6dfc6a619fa2'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Sensory memory as learning embedding representations for raw inputs, including text, \u001b[0m\n",
       "\u001b[32mimage or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted \u001b[0m\n",
       "\u001b[32mby the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent \u001b[0m\n",
       "\u001b[32mcan attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m#\\nThe external \u001b[0m\n",
       "\u001b[32mmemory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding \u001b[0m\n",
       "\u001b[32mrepresentation of information into a vector store database that can support fast maximum inner-product search \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. To optimize the retrieval speed, the common choice is the approximate nearest neighbors \u001b[0m\u001b[32m(\u001b[0m\u001b[32mANN\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\u200b \u001b[0m\n",
       "\u001b[32malgorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge \u001b[0m\n",
       "\u001b[32mspeedup.\\nA couple common choices of ANN algorithms for fast MIPS:'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'a8569ff5-e126-4153-afcb-b68b5a24c9e0'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Memory stream: is a long-term memory module \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexternal database\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that records a \u001b[0m\n",
       "\u001b[32mcomprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly \u001b[0m\n",
       "\u001b[32mprovided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval \u001b[0m\n",
       "\u001b[32mmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and \u001b[0m\n",
       "\u001b[32mimportance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask \u001b[0m\n",
       "\u001b[32mLM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: \u001b[0m\n",
       "\u001b[32msynthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are \u001b[0m\n",
       "\u001b[32mhigher-level summaries of past events \u001b[0m\u001b[32m(\u001b[0m\u001b[32m<- note that this is a bit different from self-reflection above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'e7b1ed62-8c0b-4325-a3d6-61cf8283c74d'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Short-term memory: I would consider all the in-context learning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee Prompt Engineering\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mas utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the \u001b[0m\n",
       "\u001b[32mcapability to retain and recall \u001b[0m\u001b[32m(\u001b[0m\u001b[32minfinite\u001b[0m\u001b[32m)\u001b[0m\u001b[32m information over extended periods, often by leveraging an external \u001b[0m\n",
       "\u001b[32mvector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that\u001b[0m\n",
       "\u001b[32mis missing from the model weights \u001b[0m\u001b[32m(\u001b[0m\u001b[32moften hard to change after pre-training\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, including current information, code \u001b[0m\n",
       "\u001b[32mexecution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered \u001b[0m\n",
       "\u001b[32mautonomous agent system.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'answer'\u001b[0m: \u001b[32m'Based on the provided text, here are the common types of agent memory described:\\n\\n*   **Sensory \u001b[0m\n",
       "\u001b[32mMemory:** Retains impressions of sensory information.\\n*   **Short-Term Memory:** Limited context window, used for \u001b[0m\n",
       "\u001b[32min-context learning.\\n*   **Long-Term Memory:** External vector store for fast retrieval, accessible via fast \u001b[0m\n",
       "\u001b[32mretrieval.\\n*   **Maximum Inner Product Search \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:** Stores embedding representations of information in a \u001b[0m\n",
       "\u001b[32mvector database for fast search.\\n*   **Retrieval Model:** Surfaces context to inform the agent’s behavior.\\n*   \u001b[0m\n",
       "\u001b[32m**Reflection Mechanism:** Synthesizes memories into higher-level inferences.\\n*   **Short-Term Memory:** Utilizes \u001b[0m\n",
       "\u001b[32min-context learning to learn.\\n*   **Long-Term Memory:** Retains and recalls information over extended periods, \u001b[0m\n",
       "\u001b[32mleveraging an external vector store and fast retrieval.\\n\\n'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the provided text, here are the common types of agent memory described:                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Sensory Memory:</span> Retains impressions of sensory information.                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Short-Term Memory:</span> Limited context window, used for in-context learning.                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Long-Term Memory:</span> External vector store for fast retrieval, accessible via fast retrieval.                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Maximum Inner Product Search (MIPS):</span> Stores embedding representations of information in a vector database for   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>fast search.                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Retrieval Model:</span> Surfaces context to inform the agent’s behavior.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Reflection Mechanism:</span> Synthesizes memories into higher-level inferences.                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Short-Term Memory:</span> Utilizes in-context learning to learn.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Long-Term Memory:</span> Retains and recalls information over extended periods, leveraging an external vector store and\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>fast retrieval.                                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on the provided text, here are the common types of agent memory described:                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSensory Memory:\u001b[0m Retains impressions of sensory information.                                                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mShort-Term Memory:\u001b[0m Limited context window, used for in-context learning.                                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mLong-Term Memory:\u001b[0m External vector store for fast retrieval, accessible via fast retrieval.                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mMaximum Inner Product Search (MIPS):\u001b[0m Stores embedding representations of information in a vector database for   \n",
       "\u001b[1;33m   \u001b[0mfast search.                                                                                                    \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mRetrieval Model:\u001b[0m Surfaces context to inform the agent’s behavior.                                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mReflection Mechanism:\u001b[0m Synthesizes memories into higher-level inferences.                                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mShort-Term Memory:\u001b[0m Utilizes in-context learning to learn.                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mLong-Term Memory:\u001b[0m Retains and recalls information over extended periods, leveraging an external vector store and\n",
       "\u001b[1;33m   \u001b[0mfast retrieval.                                                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are common types of agent memory?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query})\n",
    "rprint(response)\n",
    "rprint(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f5ce2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are main steps for collecting human data?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2ce150c1-a5c1-4e93-b2a9-90964cf0e99f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"With the input and the inference results, the AI assistant needs to describe the process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">straightforward manner. Then describe the task process and show your analysis and model inference results to the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user in the first person. If inference results contain a file path, must tell the user the complete file path.\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'7d33bd2a-e372-4043-b624-26663a6e4c66'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Question clustering: Embed questions and run $k$-means for clustering.\\nDemonstration </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct few-shot prompt to run inference.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'5db1fbd0-7eee-429a-af36-e9d11e8b3530'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Prompt LM with 100 most recent observations and to generate 3 most salient high-level </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning &amp; Reacting: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative agent architecture. (Image source: Park et al. 2023)\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a8569ff5-e126-4153-afcb-b68b5a24c9e0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Memory stream: is a long-term memory module (external database) that records a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model: surfaces the context to inform the agent’s behavior, according to relevance, recency and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">higher-level summaries of past events (&lt;- note that this is a bit different from self-reflection above)'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'grades'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The question asks for the main steps involved in collecting human data. The provided </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document describes a typical workflow – User Input, Task Planning, Model Selection, Task Execution, and finally, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">describing the process and results.  The document explicitly mentions the need to first answer the user's request </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and then provide a detailed analysis and inference results in the first person. Therefore, the core steps for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collecting human data are outlined within the context of the task description.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The document describes a process of collecting human data, specifically focusing on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">clustering questions. It outlines a method of selecting representative questions from each cluster based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distance to the centroid. This process likely involves generating a few-shot prompt for inference using zero-shot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CoT.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The document describes a planning and reacting agent architecture focused on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimizing believability. It outlines the agent's plan to translate reflections and environmental information into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">actions, considering relationships between agents and observations. The document also references a generative agent</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architecture and provides a question about the steps for collecting human data. Therefore, the question is relevant</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to the agent's planning and reactive behavior.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The document describes a memory stream used by an agent, focusing on recording agent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experience. It outlines how the retrieval model prioritizes relevance, recency, and importance in determining what </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information is used. The document also touches on reflection mechanisms, suggesting a higher-level synthesis of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">past events.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_web_search_required'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2ce150c1-a5c1-4e93-b2a9-90964cf0e99f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"With the input and the inference results, the AI assistant needs to describe the process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">straightforward manner. Then describe the task process and show your analysis and model inference results to the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user in the first person. If inference results contain a file path, must tell the user the complete file path.\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'7d33bd2a-e372-4043-b624-26663a6e4c66'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Question clustering: Embed questions and run $k$-means for clustering.\\nDemonstration </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct few-shot prompt to run inference.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'5db1fbd0-7eee-429a-af36-e9d11e8b3530'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Prompt LM with 100 most recent observations and to generate 3 most salient high-level </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning &amp; Reacting: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative agent architecture. (Image source: Park et al. 2023)\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a8569ff5-e126-4153-afcb-b68b5a24c9e0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Memory stream: is a long-term memory module (external database) that records a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model: surfaces the context to inform the agent’s behavior, according to relevance, recency and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">higher-level summaries of past events (&lt;- note that this is a bit different from self-reflection above)'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Okay, let’s tackle this.\\n\\nAlright, here’s my plan, broken down into a series of steps. My goal is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to optimize believability – meaning I’ll ensure my responses feel consistent and grounded in the context – while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">continuously refining my understanding of the environment.\\n\\n**1. Initial Planning &amp; Reacting – A Broad Strokes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview**\\n\\nMy plan today is to focus on optimizing believability.  I’ll be constantly considering the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relationships between agents and observations, and I’ll be using a structured approach to guide my responses.  I’m </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizing a Generative Agent Architecture – specifically referencing Park et al. 2023’s principles – to build a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory stream that records relevant experiences.  This memory will inform my actions and ensure consistency. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative agent’s role is to surface the relevant context to the agent’s behavior.\\n\\n**2. Analysis &amp; Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Inference - Let\\'s Get to Work**\\n\\nLet’s start with the question: \"What are the main steps for collecting human </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data?\"\\n\\nBased on the information available, here’s my initial analysis and inferred model inference:\\n\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Core Task:** Human data collection typically involves gathering information from individuals.\\n*   **Potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Data Types:** This could encompass a wide range of data, including:\\n    *   **Surveys &amp; Questionnaires:** </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Structured questions to elicit specific information.\\n    *   **Interviews:** More in-depth conversations, often </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yielding richer qualitative data.\\n    *   **Focus Groups:** Group discussions to gather opinions and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">perspectives.\\n    *   **User Testing:** Observing users interacting with a product or service.\\n    *   **Feedback</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Forms:** Mechanisms for users to provide direct input.\\n*   **Data Collection Methods:**\\n    *   **Online Forms:**</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Web-based forms for collecting data directly.\\n    *   **App Integrations:** Connecting with existing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications.\\n    *   **Social Media Listening:** Monitoring social media conversations.\\n    *   **User </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interviews:** Conducting in-person or remote interviews.\\n    *   **Data Logging:** Recording user behavior on a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">platform.\\n\\n\\n**3.  Model Inference - Generating High-Level Questions**\\n\\nI\\'ve used the zero-shot CoT to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generate three most salient high-level questions, considering the context of human data collection.  Here’s the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analysis:\\n\\n*   **Question 1:** \"What is the primary purpose of gathering human data?\"\\n*   **Question 2:** \"What </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are the key considerations when selecting data collection methods?\"\\n*   **Question 3:** \"How can data collected be</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to inform future product development?\"\\n\\n**4.  Response – Answering the Questions**\\n\\nOkay, let\\'s answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these questions. Here’s my response:\\n\\n\"To effectively collect human data, the process generally involves several </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">key steps. Firstly, define the objective: What specific information are you trying to gather? This will determine </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the type of data needed. Next, select appropriate data collection methods – surveys, interviews, user testing, etc.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Crucially, consider the target audience, context, and ethical considerations.  Proper onboarding and consent are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vital, ensuring users understand how their data will be used.  Finally, ensure robust data security and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">anonymization to protect privacy.\\n\\n**5.  Final Remarks &amp; Next Steps**\\n\\nI recognize that this is just a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preliminary analysis.  I\\'ll continue to refine my understanding of the environment based on the provided </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information. I will prioritize understanding the specific context of the \\'Human Data Collection\\' task.  I\\'m </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ready to begin taking actions based on this analysis.\\n\\n**As for the File Path Request:**\\n\\nThe complete file </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">path for the image source is: [Insert image source file path here - e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"path/to/park_et_al_2023.jpg\"]\\n\\n---\\n\\nLet me know if you would like me to elaborate on any of these steps or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">would like me to further refine my responses!'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[32m'What are main steps for collecting human data?'\u001b[0m,\n",
       "    \u001b[32m'documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'2ce150c1-a5c1-4e93-b2a9-90964cf0e99f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"With\u001b[0m\u001b[32m the input and the inference results, the AI assistant needs to describe the process \u001b[0m\n",
       "\u001b[32mand results. The previous stages can be formed as - User Input: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m User Input \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Task Planning: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Tasks \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Model\u001b[0m\n",
       "\u001b[32mSelection: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Model Assignment \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Task Execution: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Predictions \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. You must first answer the user's request in a\u001b[0m\n",
       "\u001b[32mstraightforward manner. Then describe the task process and show your analysis and model inference results to the \u001b[0m\n",
       "\u001b[32muser in the first person. If inference results contain a file path, must tell the user the complete file path.\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'7d33bd2a-e372-4043-b624-26663a6e4c66'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Question clustering: Embed questions and run $k$-means for clustering.\\nDemonstration \u001b[0m\n",
       "\u001b[32mselection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. \u001b[0m\n",
       "\u001b[32mSamples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are \u001b[0m\n",
       "\u001b[32mselected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and \u001b[0m\n",
       "\u001b[32mconstruct few-shot prompt to run inference.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'5db1fbd0-7eee-429a-af36-e9d11e8b3530'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"Prompt\u001b[0m\u001b[32m LM with 100 most recent observations and to generate 3 most salient high-level \u001b[0m\n",
       "\u001b[32mquestions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: \u001b[0m\n",
       "\u001b[32mtranslate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to \u001b[0m\n",
       "\u001b[32moptimize believability at the moment vs in time.\\nPrompt template: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mIntro of an agent X\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. Here is X's plan today in\u001b[0m\n",
       "\u001b[32mbroad strokes: 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nRelationships between agents and observations of one agent by another are all taken into \u001b[0m\n",
       "\u001b[32mconsideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe \u001b[0m\n",
       "\u001b[32mgenerative agent architecture. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Park et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'a8569ff5-e126-4153-afcb-b68b5a24c9e0'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Memory stream: is a long-term memory module \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexternal database\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that records a \u001b[0m\n",
       "\u001b[32mcomprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly \u001b[0m\n",
       "\u001b[32mprovided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval \u001b[0m\n",
       "\u001b[32mmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and \u001b[0m\n",
       "\u001b[32mimportance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask \u001b[0m\n",
       "\u001b[32mLM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: \u001b[0m\n",
       "\u001b[32msynthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are \u001b[0m\n",
       "\u001b[32mhigher-level summaries of past events \u001b[0m\u001b[32m(\u001b[0m\u001b[32m<- note that this is a bit different from self-reflection above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'grades'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m question asks for the main steps involved in collecting human data. The provided \u001b[0m\n",
       "\u001b[32mdocument describes a typical workflow – User Input, Task Planning, Model Selection, Task Execution, and finally, \u001b[0m\n",
       "\u001b[32mdescribing the process and results.  The document explicitly mentions the need to first answer the user's request \u001b[0m\n",
       "\u001b[32mand then provide a detailed analysis and inference results in the first person. Therefore, the core steps for \u001b[0m\n",
       "\u001b[32mcollecting human data are outlined within the context of the task description.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The document describes a process of collecting human data, specifically focusing on \u001b[0m\n",
       "\u001b[32mclustering questions. It outlines a method of selecting representative questions from each cluster based on \u001b[0m\n",
       "\u001b[32mdistance to the centroid. This process likely involves generating a few-shot prompt for inference using zero-shot \u001b[0m\n",
       "\u001b[32mCoT.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m document describes a planning and reacting agent architecture focused on \u001b[0m\n",
       "\u001b[32moptimizing believability. It outlines the agent's plan to translate reflections and environmental information into \u001b[0m\n",
       "\u001b[32mactions, considering relationships between agents and observations. The document also references a generative agent\u001b[0m\n",
       "\u001b[32marchitecture and provides a question about the steps for collecting human data. Therefore, the question is relevant\u001b[0m\n",
       "\u001b[32mto the agent's planning and reactive behavior.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocumentGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The document describes a memory stream used by an agent, focusing on recording agent \u001b[0m\n",
       "\u001b[32mexperience. It outlines how the retrieval model prioritizes relevance, recency, and importance in determining what \u001b[0m\n",
       "\u001b[32minformation is used. The document also touches on reflection mechanisms, suggesting a higher-level synthesis of \u001b[0m\n",
       "\u001b[32mpast events.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'is_web_search_required'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'2ce150c1-a5c1-4e93-b2a9-90964cf0e99f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"With\u001b[0m\u001b[32m the input and the inference results, the AI assistant needs to describe the process \u001b[0m\n",
       "\u001b[32mand results. The previous stages can be formed as - User Input: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m User Input \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Task Planning: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Tasks \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Model\u001b[0m\n",
       "\u001b[32mSelection: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Model Assignment \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, Task Execution: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Predictions \u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. You must first answer the user's request in a\u001b[0m\n",
       "\u001b[32mstraightforward manner. Then describe the task process and show your analysis and model inference results to the \u001b[0m\n",
       "\u001b[32muser in the first person. If inference results contain a file path, must tell the user the complete file path.\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'7d33bd2a-e372-4043-b624-26663a6e4c66'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Question clustering: Embed questions and run $k$-means for clustering.\\nDemonstration \u001b[0m\n",
       "\u001b[32mselection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. \u001b[0m\n",
       "\u001b[32mSamples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are \u001b[0m\n",
       "\u001b[32mselected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and \u001b[0m\n",
       "\u001b[32mconstruct few-shot prompt to run inference.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'5db1fbd0-7eee-429a-af36-e9d11e8b3530'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"Prompt\u001b[0m\u001b[32m LM with 100 most recent observations and to generate 3 most salient high-level \u001b[0m\n",
       "\u001b[32mquestions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: \u001b[0m\n",
       "\u001b[32mtranslate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to \u001b[0m\n",
       "\u001b[32moptimize believability at the moment vs in time.\\nPrompt template: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mIntro of an agent X\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. Here is X's plan today in\u001b[0m\n",
       "\u001b[32mbroad strokes: 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nRelationships between agents and observations of one agent by another are all taken into \u001b[0m\n",
       "\u001b[32mconsideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe \u001b[0m\n",
       "\u001b[32mgenerative agent architecture. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Park et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'a8569ff5-e126-4153-afcb-b68b5a24c9e0'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Memory stream: is a long-term memory module \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexternal database\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that records a \u001b[0m\n",
       "\u001b[32mcomprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly \u001b[0m\n",
       "\u001b[32mprovided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval \u001b[0m\n",
       "\u001b[32mmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and \u001b[0m\n",
       "\u001b[32mimportance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask \u001b[0m\n",
       "\u001b[32mLM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: \u001b[0m\n",
       "\u001b[32msynthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are \u001b[0m\n",
       "\u001b[32mhigher-level summaries of past events \u001b[0m\u001b[32m(\u001b[0m\u001b[32m<- note that this is a bit different from self-reflection above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'answer'\u001b[0m: \u001b[32m'Okay, let’s tackle this.\\n\\nAlright, here’s my plan, broken down into a series of steps. My goal is \u001b[0m\n",
       "\u001b[32mto optimize believability – meaning I’ll ensure my responses feel consistent and grounded in the context – while \u001b[0m\n",
       "\u001b[32mcontinuously refining my understanding of the environment.\\n\\n**1. Initial Planning & Reacting – A Broad Strokes \u001b[0m\n",
       "\u001b[32mOverview**\\n\\nMy plan today is to focus on optimizing believability.  I’ll be constantly considering the \u001b[0m\n",
       "\u001b[32mrelationships between agents and observations, and I’ll be using a structured approach to guide my responses.  I’m \u001b[0m\n",
       "\u001b[32mutilizing a Generative Agent Architecture – specifically referencing Park et al. 2023’s principles – to build a \u001b[0m\n",
       "\u001b[32mmemory stream that records relevant experiences.  This memory will inform my actions and ensure consistency. The \u001b[0m\n",
       "\u001b[32mgenerative agent’s role is to surface the relevant context to the agent’s behavior.\\n\\n**2. Analysis & Model \u001b[0m\n",
       "\u001b[32mInference - Let\\'s Get to Work**\\n\\nLet’s start with the question: \"What are the main steps for collecting human \u001b[0m\n",
       "\u001b[32mdata?\"\\n\\nBased on the information available, here’s my initial analysis and inferred model inference:\\n\\n*   \u001b[0m\n",
       "\u001b[32m**Core Task:** Human data collection typically involves gathering information from individuals.\\n*   **Potential \u001b[0m\n",
       "\u001b[32mData Types:** This could encompass a wide range of data, including:\\n    *   **Surveys & Questionnaires:** \u001b[0m\n",
       "\u001b[32mStructured questions to elicit specific information.\\n    *   **Interviews:** More in-depth conversations, often \u001b[0m\n",
       "\u001b[32myielding richer qualitative data.\\n    *   **Focus Groups:** Group discussions to gather opinions and \u001b[0m\n",
       "\u001b[32mperspectives.\\n    *   **User Testing:** Observing users interacting with a product or service.\\n    *   **Feedback\u001b[0m\n",
       "\u001b[32mForms:** Mechanisms for users to provide direct input.\\n*   **Data Collection Methods:**\\n    *   **Online Forms:**\u001b[0m\n",
       "\u001b[32mWeb-based forms for collecting data directly.\\n    *   **App Integrations:** Connecting with existing \u001b[0m\n",
       "\u001b[32mapplications.\\n    *   **Social Media Listening:** Monitoring social media conversations.\\n    *   **User \u001b[0m\n",
       "\u001b[32mInterviews:** Conducting in-person or remote interviews.\\n    *   **Data Logging:** Recording user behavior on a \u001b[0m\n",
       "\u001b[32mplatform.\\n\\n\\n**3.  Model Inference - Generating High-Level Questions**\\n\\nI\\'ve used the zero-shot CoT to \u001b[0m\n",
       "\u001b[32mgenerate three most salient high-level questions, considering the context of human data collection.  Here’s the \u001b[0m\n",
       "\u001b[32manalysis:\\n\\n*   **Question 1:** \"What is the primary purpose of gathering human data?\"\\n*   **Question 2:** \"What \u001b[0m\n",
       "\u001b[32mare the key considerations when selecting data collection methods?\"\\n*   **Question 3:** \"How can data collected be\u001b[0m\n",
       "\u001b[32mused to inform future product development?\"\\n\\n**4.  Response – Answering the Questions**\\n\\nOkay, let\\'s answer \u001b[0m\n",
       "\u001b[32mthese questions. Here’s my response:\\n\\n\"To effectively collect human data, the process generally involves several \u001b[0m\n",
       "\u001b[32mkey steps. Firstly, define the objective: What specific information are you trying to gather? This will determine \u001b[0m\n",
       "\u001b[32mthe type of data needed. Next, select appropriate data collection methods – surveys, interviews, user testing, etc.\u001b[0m\n",
       "\u001b[32mCrucially, consider the target audience, context, and ethical considerations.  Proper onboarding and consent are \u001b[0m\n",
       "\u001b[32mvital, ensuring users understand how their data will be used.  Finally, ensure robust data security and \u001b[0m\n",
       "\u001b[32manonymization to protect privacy.\\n\\n**5.  Final Remarks & Next Steps**\\n\\nI recognize that this is just a \u001b[0m\n",
       "\u001b[32mpreliminary analysis.  I\\'ll continue to refine my understanding of the environment based on the provided \u001b[0m\n",
       "\u001b[32minformation. I will prioritize understanding the specific context of the \\'Human Data Collection\\' task.  I\\'m \u001b[0m\n",
       "\u001b[32mready to begin taking actions based on this analysis.\\n\\n**As for the File Path Request:**\\n\\nThe complete file \u001b[0m\n",
       "\u001b[32mpath for the image source is: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mInsert image source file path here - e.g., \u001b[0m\n",
       "\u001b[32m\"path/to/park_et_al_2023.jpg\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n---\\n\\nLet me know if you would like me to elaborate on any of these steps or \u001b[0m\n",
       "\u001b[32mwould like me to further refine my responses!'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Okay, let’s tackle this.                                                                                           \n",
       "\n",
       "Alright, here’s my plan, broken down into a series of steps. My goal is to optimize believability – meaning I’ll   \n",
       "ensure my responses feel consistent and grounded in the context – while continuously refining my understanding of  \n",
       "the environment.                                                                                                   \n",
       "\n",
       "<span style=\"font-weight: bold\">1. Initial Planning &amp; Reacting – A Broad Strokes Overview</span>                                                          \n",
       "\n",
       "My plan today is to focus on optimizing believability.  I’ll be constantly considering the relationships between   \n",
       "agents and observations, and I’ll be using a structured approach to guide my responses.  I’m utilizing a Generative\n",
       "Agent Architecture – specifically referencing Park et al. 2023’s principles – to build a memory stream that records\n",
       "relevant experiences.  This memory will inform my actions and ensure consistency. The generative agent’s role is to\n",
       "surface the relevant context to the agent’s behavior.                                                              \n",
       "\n",
       "<span style=\"font-weight: bold\">2. Analysis &amp; Model Inference - Let's Get to Work</span>                                                                  \n",
       "\n",
       "Let’s start with the question: \"What are the main steps for collecting human data?\"                                \n",
       "\n",
       "Based on the information available, here’s my initial analysis and inferred model inference:                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Core Task:</span> Human data collection typically involves gathering information from individuals.                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Potential Data Types:</span> This could encompass a wide range of data, including:                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Surveys &amp; Questionnaires:</span> Structured questions to elicit specific information.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Interviews:</span> More in-depth conversations, often yielding richer qualitative data.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Focus Groups:</span> Group discussions to gather opinions and perspectives.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">User Testing:</span> Observing users interacting with a product or service.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Feedback Forms:</span> Mechanisms for users to provide direct input.                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Data Collection Methods:</span>                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Online Forms:</span> Web-based forms for collecting data directly.                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">App Integrations:</span> Connecting with existing applications.                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Social Media Listening:</span> Monitoring social media conversations.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">User Interviews:</span> Conducting in-person or remote interviews.                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Data Logging:</span> Recording user behavior on a platform.                                                         \n",
       "\n",
       "<span style=\"font-weight: bold\">3.  Model Inference - Generating High-Level Questions</span>                                                              \n",
       "\n",
       "I've used the zero-shot CoT to generate three most salient high-level questions, considering the context of human  \n",
       "data collection.  Here’s the analysis:                                                                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question 1:</span> \"What is the primary purpose of gathering human data?\"                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question 2:</span> \"What are the key considerations when selecting data collection methods?\"                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question 3:</span> \"How can data collected be used to inform future product development?\"                              \n",
       "\n",
       "<span style=\"font-weight: bold\">4.  Response – Answering the Questions</span>                                                                             \n",
       "\n",
       "Okay, let's answer these questions. Here’s my response:                                                            \n",
       "\n",
       "\"To effectively collect human data, the process generally involves several key steps. Firstly, define the          \n",
       "objective: What specific information are you trying to gather? This will determine the type of data needed. Next,  \n",
       "select appropriate data collection methods – surveys, interviews, user testing, etc.  Crucially, consider the      \n",
       "target audience, context, and ethical considerations.  Proper onboarding and consent are vital, ensuring users     \n",
       "understand how their data will be used.  Finally, ensure robust data security and anonymization to protect privacy.\n",
       "\n",
       "<span style=\"font-weight: bold\">5.  Final Remarks &amp; Next Steps</span>                                                                                     \n",
       "\n",
       "I recognize that this is just a preliminary analysis.  I'll continue to refine my understanding of the environment \n",
       "based on the provided information. I will prioritize understanding the specific context of the 'Human Data         \n",
       "Collection' task.  I'm ready to begin taking actions based on this analysis.                                       \n",
       "\n",
       "<span style=\"font-weight: bold\">As for the File Path Request:</span>                                                                                      \n",
       "\n",
       "The complete file path for the image source is: [Insert image source file path here - e.g.,                        \n",
       "\"path/to/park_et_al_2023.jpg\"]                                                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "Let me know if you would like me to elaborate on any of these steps or would like me to further refine my          \n",
       "responses!                                                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Okay, let’s tackle this.                                                                                           \n",
       "\n",
       "Alright, here’s my plan, broken down into a series of steps. My goal is to optimize believability – meaning I’ll   \n",
       "ensure my responses feel consistent and grounded in the context – while continuously refining my understanding of  \n",
       "the environment.                                                                                                   \n",
       "\n",
       "\u001b[1m1. Initial Planning & Reacting – A Broad Strokes Overview\u001b[0m                                                          \n",
       "\n",
       "My plan today is to focus on optimizing believability.  I’ll be constantly considering the relationships between   \n",
       "agents and observations, and I’ll be using a structured approach to guide my responses.  I’m utilizing a Generative\n",
       "Agent Architecture – specifically referencing Park et al. 2023’s principles – to build a memory stream that records\n",
       "relevant experiences.  This memory will inform my actions and ensure consistency. The generative agent’s role is to\n",
       "surface the relevant context to the agent’s behavior.                                                              \n",
       "\n",
       "\u001b[1m2. Analysis & Model Inference - Let's Get to Work\u001b[0m                                                                  \n",
       "\n",
       "Let’s start with the question: \"What are the main steps for collecting human data?\"                                \n",
       "\n",
       "Based on the information available, here’s my initial analysis and inferred model inference:                       \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mCore Task:\u001b[0m Human data collection typically involves gathering information from individuals.                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mPotential Data Types:\u001b[0m This could encompass a wide range of data, including:                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mSurveys & Questionnaires:\u001b[0m Structured questions to elicit specific information.                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mInterviews:\u001b[0m More in-depth conversations, often yielding richer qualitative data.                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mFocus Groups:\u001b[0m Group discussions to gather opinions and perspectives.                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mUser Testing:\u001b[0m Observing users interacting with a product or service.                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mFeedback Forms:\u001b[0m Mechanisms for users to provide direct input.                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mData Collection Methods:\u001b[0m                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mOnline Forms:\u001b[0m Web-based forms for collecting data directly.                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mApp Integrations:\u001b[0m Connecting with existing applications.                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mSocial Media Listening:\u001b[0m Monitoring social media conversations.                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mUser Interviews:\u001b[0m Conducting in-person or remote interviews.                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mData Logging:\u001b[0m Recording user behavior on a platform.                                                         \n",
       "\n",
       "\u001b[1m3.  Model Inference - Generating High-Level Questions\u001b[0m                                                              \n",
       "\n",
       "I've used the zero-shot CoT to generate three most salient high-level questions, considering the context of human  \n",
       "data collection.  Here’s the analysis:                                                                             \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion 1:\u001b[0m \"What is the primary purpose of gathering human data?\"                                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion 2:\u001b[0m \"What are the key considerations when selecting data collection methods?\"                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion 3:\u001b[0m \"How can data collected be used to inform future product development?\"                              \n",
       "\n",
       "\u001b[1m4.  Response – Answering the Questions\u001b[0m                                                                             \n",
       "\n",
       "Okay, let's answer these questions. Here’s my response:                                                            \n",
       "\n",
       "\"To effectively collect human data, the process generally involves several key steps. Firstly, define the          \n",
       "objective: What specific information are you trying to gather? This will determine the type of data needed. Next,  \n",
       "select appropriate data collection methods – surveys, interviews, user testing, etc.  Crucially, consider the      \n",
       "target audience, context, and ethical considerations.  Proper onboarding and consent are vital, ensuring users     \n",
       "understand how their data will be used.  Finally, ensure robust data security and anonymization to protect privacy.\n",
       "\n",
       "\u001b[1m5.  Final Remarks & Next Steps\u001b[0m                                                                                     \n",
       "\n",
       "I recognize that this is just a preliminary analysis.  I'll continue to refine my understanding of the environment \n",
       "based on the provided information. I will prioritize understanding the specific context of the 'Human Data         \n",
       "Collection' task.  I'm ready to begin taking actions based on this analysis.                                       \n",
       "\n",
       "\u001b[1mAs for the File Path Request:\u001b[0m                                                                                      \n",
       "\n",
       "The complete file path for the image source is: [Insert image source file path here - e.g.,                        \n",
       "\"path/to/park_et_al_2023.jpg\"]                                                                                     \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "Let me know if you would like me to elaborate on any of these steps or would like me to further refine my          \n",
       "responses!                                                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are main steps for collecting human data?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query})\n",
    "rprint(response)\n",
    "rprint(Markdown(response[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
